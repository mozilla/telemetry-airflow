{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "title: \"Bug 1291340 - Import sync log data\"\n",
    "authors:\n",
    "- mreid-moz\n",
    "tags:\n",
    "- sync\n",
    "- etl\n",
    "created_at: 2016-11-15\n",
    "updated_at: 2016-11-15\n",
    "tldr: Read, convert, and store sync log data to Parquet form per [bug 1291340](https://bugzilla.mozilla.org/show_bug.cgi?id=1291340).\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bug 1291340 - Import sync log data\n",
    "\n",
    "Read, convert, and store sync log data to Parquet form per [bug 1291340](https://bugzilla.mozilla.org/show_bug.cgi?id=1291340).\n",
    "\n",
    "Conversion code is ported from the [smt repo](https://github.com/dannycoates/smt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from datetime import datetime as dt, timedelta, date\n",
    "from moztelemetry.dataset import Dataset\n",
    "from os import environ\n",
    "\n",
    "# Determine run parameters\n",
    "source_bucket = 'net-mozaws-prod-us-west-2-pipeline-analysis'\n",
    "dest_bucket = source_bucket\n",
    "dest_s3_prefix = \"s3://{}/mreid\".format(dest_bucket)\n",
    "\n",
    "if \"bucket\" in os.environ:\n",
    "    dest_bucket = environ[\"bucket\"]\n",
    "    dest_s3_prefix = \"s3://{}\".format(dest_bucket)\n",
    "\n",
    "yesterday = dt.strftime(dt.utcnow() - timedelta(1), \"%Y%m%d\")\n",
    "\n",
    "# Default to running for \"yesterday\" unless we've been given a\n",
    "# specific date via the environment.\n",
    "target_day = environ.get(\"date\", yesterday)\n",
    "print \"Running import for {}\".format(target_day)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the source log data\n",
    "\n",
    "The sync data on S3 is stored in framed heka format, and is read using the `Dataset` API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read the source data\n",
    "schema = []\n",
    "target_prefix = 'sync-metrics/data'\n",
    "sync = Dataset(source_bucket, schema, prefix=target_prefix)\n",
    "\n",
    "# The sync data on S3 does not have a proper \"date\" dimension, but the date is encoded \n",
    "# in the key names themselves.\n",
    "# Fetch the summaries and filter the list to the target day.\n",
    "summary_prefix = \"{}/{}\".format(target_prefix, target_day)\n",
    "sync_summaries = [ s for s in sync.summaries(sc) if s['key'].startswith(summary_prefix) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom heka decoder\n",
    "\n",
    "The standard heka decoder assumes (based on Telemetry data) that all fields whose names have a `.` in them contain nested json strings. This is not true for sync log messages, which have fields such as `syncstorage.storage.sql.db.execute` with simple scalar values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import ssl\n",
    "from moztelemetry.heka.message_parser import unpack\n",
    "\n",
    "# Custom decoder for sync messages since we can have scalar fields with dots in their names.\n",
    "def sync_decoder(message):\n",
    "    try:\n",
    "        for record, total_bytes in unpack(message):\n",
    "            result = {}\n",
    "            result[\"meta\"] = {\n",
    "                \"Timestamp\": record.message.timestamp,\n",
    "                \"Type\":      record.message.type,\n",
    "                \"Hostname\":  record.message.hostname,\n",
    "            }\n",
    "            for field in record.message.fields:\n",
    "                name = field.name\n",
    "                value = field.value_string\n",
    "                if field.value_type == 1:\n",
    "                    # TODO: handle bytes in a way that doesn't cause problems with JSON\n",
    "                    # value = field.value_bytes\n",
    "                    continue\n",
    "                elif field.value_type == 2:\n",
    "                    value = field.value_integer\n",
    "                elif field.value_type == 3:\n",
    "                    value = field.value_double\n",
    "                elif field.value_type == 4:\n",
    "                    value = field.value_bool\n",
    "\n",
    "                result[name] = value[0] if len(value) else \"\"\n",
    "\n",
    "            yield result\n",
    "\n",
    "    except ssl.SSLError:\n",
    "        pass  # https://github.com/boto/boto/issues/2830\n",
    "\n",
    "sync_records = sync.records(sc, decode=sync_decoder, summaries=sync_summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# What do the records look like?\n",
    "\n",
    "# Example heka message:\n",
    "#Timestamp: 2016-10-28 15:11:45.98653696 -0300 ADT\n",
    "#Type: mozsvc.metrics\n",
    "#Hostname: ip-172-31-39-11\n",
    "#Pid: 11383\n",
    "#UUID: 155866c8-cc58-4048-a58c-6226c620fc57\n",
    "#Logger: Sync-1_5\n",
    "#Payload:\n",
    "#EnvVersion: 1\n",
    "#Severity: 7\n",
    "#Fields: [name:\"remoteAddressChain\" representation:\"\" value_string:\"\" value_string:\"\"  \n",
    "#         name:\"path\" value_string:\"https://host/ver/somenum/storage/tabs\"  \n",
    "#         name:\"fxa_uid\" value_string:\"some_id\"  \n",
    "#         name:\"user_agent_version\" value_type:DOUBLE value_double:49  \n",
    "#         name:\"user_agent_os\" value_string:\"Windows 7\"  \n",
    "#         name:\"device_id\" value_string:\"some_device_id\"  \n",
    "#         name:\"method\" value_string:\"POST\"  \n",
    "#         name:\"user_agent_browser\" value_string:\"Firefox\"  \n",
    "#         name:\"name\" value_string:\"mozsvc.metrics\"  \n",
    "#         name:\"request_time\" value_type:DOUBLE value_double:0.003030061721801758  \n",
    "#         name:\"code\" value_type:DOUBLE value_double:200 \n",
    "#        ]\n",
    "\n",
    "# Example record:\n",
    "#sync_records.first()\n",
    "\n",
    "# {u'code': 200.0,\n",
    "#  u'device_id': u'some_device_id',\n",
    "#  u'fxa_uid': u'some_id',\n",
    "#  'meta': {'Hostname': u'ip-172-31-39-11',\n",
    "#   'Timestamp': 1477678305976742912L,\n",
    "#   'Type': u'mozsvc.metrics'},\n",
    "#  u'method': u'GET',\n",
    "#  u'name': u'mozsvc.metrics',\n",
    "#  u'path': u'https://host/ver/somenum/storage/crypto/keys',\n",
    "#  u'remoteAddressChain': u'',\n",
    "#  u'request_time': 0.017612934112548828,\n",
    "#  u'syncstorage.storage.sql.db.execute': 0.014925241470336914,\n",
    "#  u'syncstorage.storage.sql.pool.get': 5.221366882324219e-05,\n",
    "#  u'user_agent_browser': u'Firefox',\n",
    "#  u'user_agent_os': u'Windows 7',\n",
    "#  u'user_agent_version': 49.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convert data. Code ported from https://github.com/dannycoates/smt\n",
    "import re\n",
    "import hashlib\n",
    "import math\n",
    "from pyspark.sql import Row\n",
    "\n",
    "def sha_prefix(v):\n",
    "    h = hashlib.sha256()\n",
    "    h.update(v)\n",
    "    return h.hexdigest()[0:32]\n",
    "\n",
    "path_uid = re.compile(\"(\\d+)\\/storage\\/\")\n",
    "path_bucket = re.compile(\"\\d+\\/storage\\/(\\w+)\")\n",
    "\n",
    "def getUid(path):\n",
    "    if path is None:\n",
    "        return None\n",
    "    match = re.search(path_uid, path)\n",
    "    if match is not None:\n",
    "        uid = match.group(1)\n",
    "        return sha_prefix(uid)\n",
    "    return None\n",
    "\n",
    "def deriveDeviceId(uid, agent):\n",
    "    if uid is None:\n",
    "        return None\n",
    "    return sha_prefix(\"{}{}\".format(uid, agent))\n",
    "\n",
    "SyncRow = Row(\"uid\", \"s_uid\", \"dev\", \"s_dev\", \"ts\", \"method\", \"code\", \n",
    "              \"bucket\", \"t\", \"ua_browser\", \"ua_version\", \"ua_os\", \"host\")\n",
    "\n",
    "def convert(msg):\n",
    "    bmatch = re.search(path_bucket, msg.get(\"path\", \"\"))\n",
    "    if bmatch is None:\n",
    "        return None\n",
    "    bucket = bmatch.group(1)\n",
    "    \n",
    "    uid = msg.get(\"fxa_uid\")\n",
    "    synth_uid = getUid(msg.get(\"path\"))\n",
    "    dev = msg.get(\"device_id\")\n",
    "    synth_dev = deriveDeviceId(synth_uid,\n",
    "        \"{}{}{}\".format(\n",
    "            msg.get(\"user_agent_browser\", \"\"),\n",
    "            msg.get(\"user_agent_version\", \"\"),\n",
    "            msg.get(\"user_agent_os\", \"\"))\n",
    "      )\n",
    "    \n",
    "    code = 200\n",
    "    # support modern mozlog's use of errno for http status\n",
    "    errno = msg.get(\"errno\")\n",
    "    if errno is not None:\n",
    "        if errno == 0: # success\n",
    "            code = 200\n",
    "        else:\n",
    "            code = errno\n",
    "    else:\n",
    "        code = msg.get(\"code\")\n",
    "        if code is not None:\n",
    "            code = int(code)\n",
    "    \n",
    "    t = msg.get(\"t\", 0)\n",
    "    if t == 0:\n",
    "        t = math.floor(msg.get(\"request_time\", 0) * 1000)\n",
    "    if t is None:\n",
    "        t = 0\n",
    "\n",
    "    converted = SyncRow(\n",
    "        (uid or synth_uid),\n",
    "        synth_uid,\n",
    "        (dev or synth_dev),\n",
    "        synth_dev,\n",
    "        msg.get(\"meta\").get(\"Timestamp\"),\n",
    "        msg.get(\"method\"),\n",
    "        code,\n",
    "        bucket,\n",
    "        t,\n",
    "        msg.get(\"user_agent_browser\"),\n",
    "        msg.get(\"user_agent_version\"),\n",
    "        msg.get(\"user_agent_os\"),\n",
    "        msg.get(\"meta\").get(\"Hostname\"),\n",
    "    )\n",
    "    return converted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "converted = sync_records.map(lambda x: convert(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "converted = converted.filter(lambda x: x is not None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sync_df = sqlContext.createDataFrame(converted)\n",
    "sync_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Determine if we need to repartition.\n",
    "# A record is something like 112 bytes, so figure out how many partitions\n",
    "# we need to end up with reasonably-sized files.\n",
    "records_per_partition = 2500000\n",
    "total_records = sync_df.count()\n",
    "print \"Found {} sync records\".format(total_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "num_partitions = int(math.ceil(float(total_records) / records_per_partition))\n",
    "\n",
    "if num_partitions != sync_df.rdd.getNumPartitions():\n",
    "    print \"Repartitioning with {} partitions\".format(num_partitions)\n",
    "    sync_df = sync_df.repartition(num_partitions)\n",
    "\n",
    "# Store data\n",
    "sync_log_s3path = \"{}/sync_log/v1/day={}\".format(dest_s3_prefix, target_day)\n",
    "sync_df.write.parquet(sync_log_s3path, mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Transform, compute and store rollups\n",
    "sync_df.registerTempTable(\"sync\")\n",
    "sql_transform = '''\n",
    "  select\n",
    "    uid,\n",
    "    dev,\n",
    "    ts,\n",
    "    t,\n",
    "    case \n",
    "     when substring(ua_os,0,7) in ('iPad', 'iPod', 'iPhone') then 'ios'\n",
    "     when substring(ua_os,0,7) = 'Android' then 'android'\n",
    "     when substring(ua_os,0,7) = 'Windows' then 'windows'\n",
    "     when substring(ua_os,0,7) = 'Macinto' then 'mac'\n",
    "     when substring(ua_os,0,7) = 'Linux' then 'linux'\n",
    "     when ua_os is null then 'unknown'\n",
    "     else 'other'\n",
    "    end as ua_os,\n",
    "    ua_browser,\n",
    "    ua_version,\n",
    "    case method when 'POST' then 1 end as posts,\n",
    "    case method when 'GET' then 1 end as gets,\n",
    "    case method when 'PUT' then 1 end as puts,\n",
    "    case method when 'DELETE' then 1 end as dels,\n",
    "    case when code < 300 then 1 end as aoks,\n",
    "    case when code > 399 and code < 500 then 1 end as oops,\n",
    "    case when code > 499 and code < 999 then 1 end as fups,\n",
    "    case when bucket = 'clients' and method = 'GET' then 1 end as r_clients,\n",
    "    case when bucket = 'crypto' and method = 'GET' then 1 end as r_crypto,\n",
    "    case when bucket = 'forms' and method = 'GET' then 1 end as r_forms,\n",
    "    case when bucket = 'history' and method = 'GET' then 1 end as r_history,\n",
    "    case when bucket = 'keys' and method = 'GET' then 1 end as r_keys,\n",
    "    case when bucket = 'meta' and method = 'GET' then 1 end as r_meta,\n",
    "    case when bucket = 'bookmarks' and method = 'GET' then 1 end as r_bookmarks,\n",
    "    case when bucket = 'prefs' and method = 'GET' then 1 end as r_prefs,\n",
    "    case when bucket = 'tabs' and method = 'GET' then 1 end as r_tabs,\n",
    "    case when bucket = 'passwords' and method = 'GET' then 1 end as r_passwords,\n",
    "    case when bucket = 'addons' and method = 'GET' then 1 end as r_addons,\n",
    "    case when bucket = 'clients' and method = 'POST' then 1 end as w_clients,\n",
    "    case when bucket = 'crypto' and method = 'POST' then 1 end as w_crypto,\n",
    "    case when bucket = 'forms' and method = 'POST' then 1 end as w_forms,\n",
    "    case when bucket = 'history' and method = 'POST' then 1 end as w_history,\n",
    "    case when bucket = 'keys' and method = 'POST' then 1 end as w_keys,\n",
    "    case when bucket = 'meta' and method = 'POST' then 1 end as w_meta,\n",
    "    case when bucket = 'bookmarks' and method = 'POST' then 1 end as w_bookmarks,\n",
    "    case when bucket = 'prefs' and method = 'POST' then 1 end as w_prefs,\n",
    "    case when bucket = 'tabs' and method = 'POST' then 1 end as w_tabs,\n",
    "    case when bucket = 'passwords' and method = 'POST' then 1 end as w_passwords,\n",
    "    case when bucket = 'addons' and method = 'POST' then 1 end as w_addons\n",
    "  from sync\n",
    "'''\n",
    "\n",
    "transformed = sqlContext.sql(sql_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "transformed.registerTempTable(\"tx\")\n",
    "\n",
    "sql_device_activity = '''\n",
    "  select\n",
    "    uid,\n",
    "    dev,\n",
    "    max(ua_os) as ua_os,\n",
    "    max(ua_browser) as ua_browser,\n",
    "    max(ua_version) as ua_version,\n",
    "    min(t) as min_t,\n",
    "    max(t) as max_t,\n",
    "    sum(posts) as posts,\n",
    "    sum(gets) as gets,\n",
    "    sum(puts) as puts,\n",
    "    sum(dels) as dels,\n",
    "    sum(aoks) as aoks,\n",
    "    sum(oops) as oops,\n",
    "    sum(fups) as fups,\n",
    "    sum(r_clients) as r_clients,\n",
    "    sum(r_crypto) as r_crypto,\n",
    "    sum(r_forms) as r_forms,\n",
    "    sum(r_history) as r_history,\n",
    "    sum(r_keys) as r_keys,\n",
    "    sum(r_meta) as r_meta,\n",
    "    sum(r_bookmarks) as r_bookmarks,\n",
    "    sum(r_prefs) as r_prefs,\n",
    "    sum(r_tabs) as r_tabs,\n",
    "    sum(r_passwords) as r_passwords,\n",
    "    sum(r_addons) as r_addons,\n",
    "    sum(w_clients) as w_clients,\n",
    "    sum(w_crypto) as w_crypto,\n",
    "    sum(w_forms) as w_forms,\n",
    "    sum(w_history) as w_history,\n",
    "    sum(w_keys) as w_keys,\n",
    "    sum(w_meta) as w_meta,\n",
    "    sum(w_bookmarks) as w_bookmarks,\n",
    "    sum(w_prefs) as w_prefs,\n",
    "    sum(w_tabs) as w_tabs,\n",
    "    sum(w_passwords) as w_passwords,\n",
    "    sum(w_addons) as w_addons\n",
    "  from tx group by uid, dev\n",
    "'''\n",
    "rolled_up = sqlContext.sql(sql_device_activity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Store device activity rollups\n",
    "sync_log_device_activity_s3base = \"{}/sync_log_device_activity/v1\".format(dest_s3_prefix)\n",
    "sync_log_device_activity_s3path = \"{}/day={}\".format(sync_log_device_activity_s3base, target_day)\n",
    "\n",
    "# TODO: Do we need to repartition?\n",
    "rolled_up.repartition(5).write.parquet(sync_log_device_activity_s3path, mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_device_counts(device_activity, target_day):\n",
    "    device_activity.registerTempTable(\"device_activity\")\n",
    "    df = \"%Y%m%d\"\n",
    "    last_week_date = dt.strptime(target_day, df) - timedelta(7)\n",
    "    last_week = dt.strftime(last_week_date, df)\n",
    "    sql_device_counts = \"\"\"\n",
    "        select\n",
    "          uid,\n",
    "          count(distinct dev) as devs\n",
    "        from\n",
    "          (select\n",
    "            uid,\n",
    "            dev\n",
    "          from device_activity\n",
    "          where uid in\n",
    "            (select distinct(uid) from device_activity where day = '{}')\n",
    "            and day > '{}'\n",
    "            and day <= '{}')\n",
    "        group by uid\n",
    "    \"\"\".format(target_day, last_week, target_day)\n",
    "\n",
    "    return sqlContext.sql(sql_device_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute and store device counts\n",
    "\n",
    "# Re-read device activity data from S3 so we can look at historic info\n",
    "device_activity = sqlContext.read.parquet(sync_log_device_activity_s3base)\n",
    "\n",
    "device_counts = compute_device_counts(device_activity, target_day)\n",
    "\n",
    "sync_log_device_counts_s3path = \"{}/sync_log_device_counts/v1/day={}\".format(dest_s3_prefix, target_day)\n",
    "device_counts.repartition(1).write.parquet(sync_log_device_counts_s3path, mode=\"overwrite\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}