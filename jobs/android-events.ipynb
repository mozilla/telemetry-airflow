{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Android Events ETL job\"\n",
    "authors:\n",
    "- Frank Bertsch\n",
    "tags:\n",
    "- mobile\n",
    "- etl\n",
    "created_at: 2017-02-17\n",
    "updated_at: 2017-02-17\n",
    "tldr: This job takes the Fennec saved session pings and transforms them, where there could be multiple events per ping.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "import os\n",
    "import pandas as pd\n",
    "import ujson as json\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "from moztelemetry import get_pings, get_pings_properties, get_one_ping_per_client\n",
    "\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take the set of pings, make sure we have actual clientIds and remove duplicate pings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def dedupe_pings(rdd):\n",
    "    return rdd.filter(lambda p: p[\"meta/clientId\"] is not None)\\\n",
    "              .map(lambda p: (p[\"meta/documentId\"], p))\\\n",
    "              .reduceByKey(lambda x, y: x)\\\n",
    "              .map(lambda x: x[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to dump each event from the pings. Do a little empty data sanitization so we don't get NoneType errors during the dump. We create a JSON array of active experiments as part of the dump."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def safe_str(obj):\n",
    "    \"\"\" return the byte string representation of obj \"\"\"\n",
    "    if obj is None:\n",
    "        return unicode(\"\")\n",
    "    return unicode(obj)\n",
    "\n",
    "def transform(ping):    \n",
    "    output = []\n",
    "\n",
    "    # These should not be None since we filter those out & ingestion process adds the data\n",
    "    clientId = ping[\"meta/clientId\"]\n",
    "    submissionDate = dt.datetime.strptime(ping[\"meta/submissionDate\"], \"%Y%m%d\")\n",
    "\n",
    "    events = ping[\"payload/UIMeasurements\"]\n",
    "    if events and isinstance(events, list):\n",
    "        for event in events:\n",
    "            if isinstance(event, dict) and \"type\" in event and event[\"type\"] == \"event\":\n",
    "                if \"timestamp\" not in event or \"action\" not in event or \"method\" not in event or \"sessions\" not in event:\n",
    "                    continue\n",
    "\n",
    "                # Verify timestamp is a long, otherwise ignore the event\n",
    "                timestamp = None\n",
    "                try:\n",
    "                    timestamp = long(event[\"timestamp\"])\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "                # Force all fields to strings\n",
    "                action = safe_str(event[\"action\"])\n",
    "                method = safe_str(event[\"method\"])\n",
    "\n",
    "                # The extras is an optional field\n",
    "                extras = unicode(\"\")\n",
    "                if \"extras\" in event and safe_str(event[\"extras\"]) is not None:\n",
    "                    extras = safe_str(event[\"extras\"])\n",
    "\n",
    "                sessions = set()\n",
    "                experiments = []\n",
    "                \n",
    "                try:\n",
    "                    for session in event[\"sessions\"]:\n",
    "                        if \"experiment.1:\" in session:\n",
    "                            experiments.append(safe_str(session[13:]))\n",
    "                        else:\n",
    "                            sessions.add(safe_str(session))\n",
    "                except TypeError:\n",
    "                    pass\n",
    "\n",
    "                output.append([clientId, submissionDate, timestamp, action, method, extras, json.dumps(list(sessions)), json.dumps(experiments)])\n",
    "\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data can have duplicate events, due to a bug in the data collection that was fixed (bug 1246973). We still need to de-dupe the events. Because pings can be archived on device and submitted on later days, we can't assume dupes only happen on the same submission day. We don't use submission date when de-duping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def dedupe_events(rdd):\n",
    "    return rdd.map(lambda p: (p[0] + safe_str(p[2]) + p[3] + p[4], p))\\\n",
    "              .reduceByKey(lambda x, y: x)\\\n",
    "              .map(lambda x: x[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a set of events from \"saved-session\" UI telemetry. Output the data to CSV or Parquet.\n",
    "\n",
    "This script is designed to loop over a range of days and output a single day for the given channels. Use explicit date ranges for backfilling, or now() - '1day' for automated runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "channels = [\"nightly\", \"aurora\", \"beta\", \"release\"]\n",
    "\n",
    "batch_date = os.environ.get('date')\n",
    "if batch_date:\n",
    "    start = end = dt.datetime.strptime(batch_date, '%Y%m%d')\n",
    "else:\n",
    "    start = start = dt.datetime.now() - dt.timedelta(1)\n",
    "\n",
    "day = start\n",
    "while day <= end:\n",
    "    for channel in channels:\n",
    "        print \"\\nchannel: \" + channel + \", date: \" + day.strftime(\"%Y%m%d\")\n",
    "\n",
    "        pings = get_pings(sc, app=\"Fennec\", channel=channel,\n",
    "                          submission_date=(day.strftime(\"%Y%m%d\"), day.strftime(\"%Y%m%d\")),\n",
    "                          build_id=(\"20100101000000\", \"99999999999999\"),\n",
    "                          fraction=1)\n",
    "\n",
    "        subset = get_pings_properties(pings, [\"meta/clientId\",\n",
    "                                              \"meta/documentId\",\n",
    "                                              \"meta/submissionDate\",\n",
    "                                              \"payload/UIMeasurements\"])\n",
    "\n",
    "        subset = dedupe_pings(subset)\n",
    "        print subset.first()\n",
    "\n",
    "        rawEvents = subset.flatMap(transform)\n",
    "        print \"\\nRaw count: \" + str(rawEvents.count())\n",
    "        print rawEvents.first()\n",
    "\n",
    "        uniqueEvents = dedupe_events(rawEvents)\n",
    "        print \"\\nUnique count: \" + str(uniqueEvents.count())\n",
    "        print uniqueEvents.first()\n",
    "\n",
    "        s3_output = \"s3n://net-mozaws-prod-us-west-2-pipeline-analysis/mobile/android_events\"\n",
    "        s3_output += \"/v1/channel=\" + channel + \"/submission=\" + day.strftime(\"%Y%m%d\") \n",
    "        schema = StructType([\n",
    "            StructField(\"clientid\", StringType(), False),\n",
    "            StructField(\"submissiondate\", TimestampType(), False),\n",
    "            StructField(\"ts\", LongType(), True),\n",
    "            StructField(\"action\", StringType(), True),\n",
    "            StructField(\"method\", StringType(), True),\n",
    "            StructField(\"extras\", StringType(), True),\n",
    "            StructField(\"sessions\", StringType(), True),\n",
    "            StructField(\"experiments\", StringType(), True)\n",
    "        ])\n",
    "        grouped = sqlContext.createDataFrame(uniqueEvents, schema)\n",
    "        grouped.coalesce(1).write.parquet(s3_output, mode=\"overwrite\")\n",
    "\n",
    "    day += dt.timedelta(1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}