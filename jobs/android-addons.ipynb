{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Android Addons ETL job\"\n",
    "authors:\n",
    "- Frank Bertsch\n",
    "tags:\n",
    "- mobile\n",
    "- etl\n",
    "created_at: 2017-02-17\n",
    "updated_at: 2017-02-17\n",
    "tldr: This job takes the Fennec saved session pings and maps them to just client, submissionDate, activeAddons, and persona.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "import os\n",
    "import pandas as pd\n",
    "import operator\n",
    "import ujson as json\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "from moztelemetry import get_pings, get_pings_properties, get_one_ping_per_client\n",
    "\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take the set of pings, make sure we have actual clientIds and remove duplicate pings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def safe_str(obj):\n",
    "    \"\"\" return the byte string representation of obj \"\"\"\n",
    "    if obj is None:\n",
    "        return unicode(\"\")\n",
    "    return unicode(obj)\n",
    "\n",
    "def dedupe_pings(rdd):\n",
    "    return rdd.filter(lambda p: p[\"meta/clientId\"] is not None)\\\n",
    "              .map(lambda p: (p[\"meta/documentId\"], p))\\\n",
    "              .reduceByKey(lambda x, y: x)\\\n",
    "              .map(lambda x: x[1])\n",
    "\n",
    "def dedupe_addons(rdd):\n",
    "    return rdd.map(lambda p: (p[0] + safe_str(p[2]) + safe_str(p[3]), p))\\\n",
    "              .reduceByKey(lambda x, y: x)\\\n",
    "              .map(lambda x: x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to dump each event from the pings. Do a little empty data sanitization so we don't get NoneType errors during the dump. We create a JSON array of active experiments as part of the dump."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def clean(s):\n",
    "    try:\n",
    "        s = s.decode(\"ascii\").strip()\n",
    "        return s if len(s) > 0 else None\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def transform(ping):    \n",
    "    output = []\n",
    "\n",
    "    # These should not be None since we filter those out & ingestion process adds the data\n",
    "    clientId = ping[\"meta/clientId\"]\n",
    "    submissionDate = dt.datetime.strptime(ping[\"meta/submissionDate\"], \"%Y%m%d\")\n",
    "\n",
    "    addonset = {}\n",
    "    addons = ping[\"environment/addons/activeAddons\"]\n",
    "    if addons is not None:\n",
    "        for addon, desc in addons.iteritems():\n",
    "            name = clean(desc.get(\"name\", None))\n",
    "            if name is not None:\n",
    "                addonset[name] = 1\n",
    "\n",
    "    persona = ping[\"environment/addons/persona\"]\n",
    "\n",
    "    if len(addonset) > 0 or persona is not None:\n",
    "        addonarray = None\n",
    "        if len(addonset) > 0:\n",
    "            addonarray = json.dumps(addonset.keys())\n",
    "        output.append([clientId, submissionDate, addonarray, persona])\n",
    "\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a set of events from \"saved-session\" UI telemetry. Output the data to CSV or Parquet.\n",
    "\n",
    "This script is designed to loop over a range of days and output a single day for the given channels. Use explicit date ranges for backfilling, or now() - '1day' for automated runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "channels = [\"nightly\", \"aurora\", \"beta\", \"release\"]\n",
    "\n",
    "batch_date = os.environ.get('date')\n",
    "if batch_date:\n",
    "    start = end = dt.datetime.strptime(batch_date, '%Y%m%d')\n",
    "else:\n",
    "    start = start = dt.datetime.now() - dt.timedelta(1)\n",
    "\n",
    "day = start\n",
    "while day <= end:\n",
    "    for channel in channels:\n",
    "        print \"\\nchannel: \" + channel + \", date: \" + day.strftime(\"%Y%m%d\")\n",
    "\n",
    "        pings = get_pings(sc, app=\"Fennec\", channel=channel,\n",
    "                          submission_date=(day.strftime(\"%Y%m%d\"), day.strftime(\"%Y%m%d\")),\n",
    "                          build_id=(\"20100101000000\", \"99999999999999\"),\n",
    "                          fraction=1)\n",
    "\n",
    "        subset = get_pings_properties(pings, [\"meta/clientId\",\n",
    "                                              \"meta/documentId\",\n",
    "                                              \"meta/submissionDate\",\n",
    "                                              \"environment/addons/activeAddons\",\n",
    "                                              \"environment/addons/persona\"])\n",
    "\n",
    "        subset = dedupe_pings(subset)\n",
    "        print subset.first()\n",
    "\n",
    "        rawAddons = subset.flatMap(transform)\n",
    "        print \"\\nrawAddons count: \" + str(rawAddons.count())\n",
    "        print rawAddons.first()\n",
    "\n",
    "        uniqueAddons = dedupe_addons(rawAddons)\n",
    "        print \"\\nuniqueAddons count: \" + str(uniqueAddons.count())\n",
    "        print uniqueAddons.first()\n",
    "\n",
    "        s3_output = \"s3n://net-mozaws-prod-us-west-2-pipeline-analysis/mobile/android_addons\"\n",
    "        s3_output += \"/v1/channel=\" + channel + \"/submission=\" + day.strftime(\"%Y%m%d\") \n",
    "        schema = StructType([\n",
    "            StructField(\"clientid\", StringType(), False),\n",
    "            StructField(\"submissiondate\", TimestampType(), False),\n",
    "            StructField(\"addons\", StringType(), True),\n",
    "            StructField(\"lwt\", StringType(), True)\n",
    "        ])\n",
    "        grouped = sqlContext.createDataFrame(uniqueAddons, schema)\n",
    "        grouped.coalesce(1).write.parquet(s3_output, mode=\"overwrite\")\n",
    "\n",
    "    day += dt.timedelta(1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}