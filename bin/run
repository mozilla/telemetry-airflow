#!/usr/bin/env bash

set -eo pipefail
set -x

# default variables
: "${PORT:=8000}"
: "${SLEEP:=1}"
: "${TRIES:=60}"

usage() {
  echo "usage: bin/run flower|web|worker|scheduler"
  exit 1
}

wait_for() {
  tries=0
  echo "Waiting for $1 to listen on $2..."
  while true; do
    [[ $tries -lt $TRIES ]] || return
    (echo > /dev/tcp/$1/$2) >/dev/null 2>&1
    result=
    [[ $? -eq 0 ]] && return
    sleep $SLEEP
    tries=$((tries + 1))
  done
}

gcp_default_extras() {
python - <<END
import json

extra = {
    "extra__google_cloud_platform__project": "dummy_project_id",
    "extra__google_cloud_platform__keyfile_dict": json.dumps({
        "type": "service_account",
        "project_id": "dummy-project",
        "private_key_id": "d8b87911500d92da857b92ab74a9d53631e8904e",
        "private_key": "-----BEGIN PRIVATE KEY-----\nMIIEogIBAAKCAQEAjj1EQOX7o14lnDRJAyFZeXGNLywPuyf/vdDt6tI2v0cTgoOH\nEAaCL5OYyXnYnLYEfQIap5sU8DcVDgiidCQv3+PKyzXPV8YPe9E2ureMZJAu48Zn\nKPzoltvLFFqAIX8yzezb3Av+tnRY495iuqTpR1x4ZKXvgVPFtOotT1dseZhNbZ2x\nmg5mNVxxsV3pVPmaRLj8CjtJI2qU3EZ2AbxTON3k4k0Q+7Gy2bG5Ef5rKbTFsp5O\nFF7MGjX7A54H7keVCc0QkEWSx/bsaRkGsnDbrZG3Kn118xJOYY/+CREURNO1FNTA\nzlJgIxN4PAOAGRXEBeXcsCZKp3MYpU75cvF5/wIDAQABAoIBAFtXkMs0ZaKFxRVI\nplJySiko+IeAfiGsEBlvYDnaAPpYxHidylBKPbQbzpQjwSzx3nQAs+lKN+oDFWxL\nszduPahDemmBBsPRFwRmWAUT9f5mcRYoxPqXXy3xu7o4W+wm6RNAtffbZBj7IlJC\n75f4ay4+fbn0rZeZmm8Rq0M2WxzB8DDiO96N3JLhPYV/uR9TeQp4pn/lUYLKRO2i\nuMBmCLl6hjaV8/0Dox0KhNICox+BXKnIUlIrDV9NanmO5dAZJPOzSHH6aJ4ovdFo\nmLdqlBw50qGm9DaYHR3pbZo17kX/42yZogrcEJMNGHjJgZna+W54gVsP6Gs4ht3G\nO+O6A4ECgYEA2oASmeAVs65VdVpBpI6Ky1PW4wFV+zMxVykbgY++6ZTg8rjRfzps\nDsmoGTaI3b/lxeiakIu96bkSr3A7r1fVuKtwSzU5jSSkONe8mWiHt4VIX+x5GiY2\nP4CZCEi3v+bVzovmXcx9Wy62h+bBWsrkYav2CrXrOSB7Byz7XduQVmECgYEApqag\nrpY7Db2HuCK3yhp08AT7itDi9ck43Zes/YB2TqXtTYsDVWVNeYcXkSh1VrQytQZW\ncPXtfmBXogGSTnjXEmpCRf+z5Bsxu57Zole5ENiBTzEcpFJpQ/5UPH8PJzWA+dB4\ncAuvgRKxnTG8LRhkrbVM8t0WJi1v1nP6DyDA7F8CgYARC/bnjGUFDK/cJPuEFB7d\n+B+GvF7x5y+NRkbAF+/kF1ppdWPa0jsF+FOmC+wnqMYLZ7dPWaeqaWb8yvvNFUQ2\nUSHErFVeHqK1UJeFPHOCLOLVoQRdtud6ktTdoZa8YQ0DPUTuwnpxN7bD6YviQnwI\n5rqeYU0FuvP+PlMqImwjYQKBgG/fbgCls1D/CcwP6mdPKW8zORWwMpwjD/yZ5LRs\n937Gnq4ugvdhwQezK5vzmAmzgFLLxV2himQLEukbuvbY4jBnisPo6v9XTiSQd7Mm\nxoLLhMr/wiWBeU7+vde6yBZfMY0CaMd24MN6JCfNinCPbo66JcTnrAXG/MvvIU/k\ngf5rAoGAfFN3JUFRcGZsQ+2qqeghcnD38iXkhBE5WL/g5CMA+v/AXvS7L67HVcTa\nV2O/KlmS53RV5MPnNZsOCyWBtohLYc+8kTAwl+DUf2ZfbzLJty6mCPrlaRaqCZ9q\nXTu2Ea7bs7gbVmhj2xcD+OPBUkITy7+XWAqwqe6C4hhwiHTDIoI=\n-----END PRIVATE KEY-----\n",
        "client_email": "test-account@dummy-project.iam.gserviceaccount.com",
        "client_id": "19327456837",
        "auth_uri": "https://accounts.google.com/o/oauth2/auth",
        "token_uri": "https://oauth2.googleapis.com/token",
        "auth_provider_x509_cert_url": "https://www.googleapis.com/oauth2/v1/certs",
        "client_x509_cert_url": "https://www.googleapis.com/robot/v1/metadata/x509/test-account%40dummy-project.iam.gserviceaccount.com"
    })
}

print(json.dumps(extra))
END
}

aws_default_extras() {
python - <<END
import json

extra = {
    "aws_access_key_id": "dummy_access_key_id",
    "aws_secret_access_key": "dummy_secret_access_key",
}

print(json.dumps(extra))
END
}

acoustic_default_extras() {
python - <<END
import json

extra = {
    "refresh_token": "dummy_refresh_token",
}

print(json.dumps(extra))
END
}


init_connections() {
    # set dummy credentials
    export AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID:-dummy_access_key_id}
    export AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY:-dummy_secret_access_key}

    airflow connections delete databricks_default

    airflow connections add databricks_default \
        --conn-type databricks \
        --conn-host https://dbc-caf9527b-e073.cloud.databricks.com \
        --conn-extra "{\"token\":\"${DB_TOKEN}\", \"host\": \"\"}"

    gcp_conn=(
      "google_cloud_airflow_dataproc"
      "google_cloud_airflow_gke"
      "google_cloud_airflow_pubsub"
      "google_cloud_derived_datasets"
      "google_cloud_glam_fenix_dev"
      "google_cloud_shared_prod"
      "google_cloud_gke_sandbox"
    )
    for conn_id in "${gcp_conn[@]}"; do
        airflow connections delete "${conn_id}"

        airflow connections add "${conn_id}" \
        --conn-type google_cloud_platform \
        --conn-extra "$(gcp_default_extras)"
    done

    aws_conn=(
      "airflow_taar_rw_s3"
      "aws_dev_iam_s3"
      "aws_data_iam_ses"
      "aws_dev_socorro_telemetry_parquet_s3"
      "aws_dev_telemetry_public_analysis_2_rw"
      "aws_prod_fx_usage_report"
      "aws_prod_probe_scraper"
      "aws_socorro_readonly_s3"
    )
    for conn_id in "${aws_conn[@]}"; do
        airflow connections delete "${conn_id}"

        airflow connections add "${conn_id}" \
        --conn-type s3 \
        --conn-extra "$(aws_default_extras)"
    done

    airflow connections delete "http_netlify_build_webhook"

    airflow connections add "http_netlify_build_webhook" \
      --conn-type http \
      --conn-host "https://httpbin.org/"

    airflow connections delete "acoustic"

    airflow connections add "acoustic" \
      --conn-type http \
      --conn-host "https://api-campaign-us-6.goacoustic.com" \
      --conn-login "" \
      --conn-password "" \
      --conn-description "Used to authenticate into Acoustic to generate data which then gets consumed by Fivetran" \
      --conn-extra "$(acoustic_default_extras)"
}

init_variables() {
    airflow variables set "bugzilla_probe_expiry_bot_api_key" "bugzilla-api-key"
    airflow variables set "app_store_connect_username" "username"
    airflow variables set "app_store_connect_password" "password"
    airflow variables set "surveygizmo_daily_attitudes_survey_id" "12345"
    airflow variables set "surveygizmo_api_token" "tokentokentoken"
    airflow variables set "surveygizmo_api_secret" "tapsekret"
    airflow variables set "jetstream_cluster_ip" "127.0.0.1"
    airflow variables set "jetstream_cluster_cert" "cert"

    airflow variables set "taar_bigtable_instance_id" "taar_bigtable_instance_id"
    airflow variables set "taar_etl_storage_bucket" "taar_etl_storage_bucket"
    airflow variables set "taar_etl_model_storage_bucket" "taar_etl_model_storage_bucket"
    airflow variables set "taar_gcp_project_id" "taar_gcp_project_id"
    airflow variables set "taar_dataflow_subnetwork" "taar_dataflow_subnetwork"
    airflow variables set "taar_dataflow_service_account_email" "taar_dataflow_service_account_email"

    airflow variables set "looker_repos_secret_git_ssh_key_b64" "looker_repos_secret_git_ssh_key_b64"
    airflow variables set "looker_api_client_id_staging" "looker_api_client_id_staging"
    airflow variables set "looker_api_client_secret_staging" "looker_api_client_secret_staging"
    airflow variables set "looker_api_client_id_prod" "looker_api_client_id_prod"
    airflow variables set "looker_api_client_secret_prod" "looker_api_client_secret_prod"
    airflow variables set "dataops_looker_github_secret_access_token" "dataops_looker_github_secret_access_token"

    airflow variables set "glean_dictionary_netlify_build_webhook_id" "status/200"
    airflow variables set "lookml_generator_release_str" "v0.0.0"

    airflow variables set "fivetran_acoustic_raw_recipient_export_connector_id" "dummy_connector_id"

    airflow variables set "fivetran_acoustic_contact_export_connector_id" "dummy_connector_id"
    airflow variables set "fivetran_acoustic_contact_export_list_id" "00000"

    airflow variables set "slack_secret_token" "slack_secret_token"

    airflow variables set "Dev_glam_secret__database_url" "Dev_glam_secret__database_url"
    airflow variables set "Prod_glam_secret__database_url" "Prod_glam_secret__database_url"
    airflow variables set "Dev_glam_secret__django_secret_key" "Dev_glam_secret__django_secret_key"
    airflow variables set "Prod_glam_secret__django_secret_key" "Prod_glam_secret__django_secret_key"
    airflow variables set "Dev_glam_project" "Dev_glam_project"
    airflow variables set "Prod_glam_project" "Prod_glam_project"
}

[ $# -lt 1 ] && usage

# Only wait for backend services in development
# http://stackoverflow.com/a/13864829
[ ! -z ${DEVELOPMENT+check} ] && wait_for db 3306 && wait_for redis 6379

case $1 in
  flower)
    exec airflow celery flower
    ;;
  web)
    airflow db init
    airflow db upgrade

    # Only init connections in dev
    [ ! -z ${DEVELOPMENT+check} ] && init_connections && init_variables
    echo "[testing_stage_0]: Initialization complete!"

    exec airflow webserver -p ${PORT} --workers 4
    ;;
  worker)
    exec airflow celery worker
    ;;
  scheduler)
    exec airflow scheduler
    ;;
  *)
    exec "$@"
    ;;
esac
