#!/usr/bin/env bash

set -eo pipefail

# default variables
: "${PORT:=8000}"
: "${SLEEP:=1}"
: "${TRIES:=60}"

usage() {
  echo "usage: bin/run flower|web|worker|scheduler"
  exit 1
}

wait_for() {
  tries=0
  echo "Waiting for $1 to listen on $2..."
  while true; do
    [[ $tries -lt $TRIES ]] || return
    (echo > /dev/tcp/$1/$2) >/dev/null 2>&1
    result=
    [[ $? -eq 0 ]] && return
    sleep $SLEEP
    tries=$((tries + 1))
  done
}

init_connections() {
    airflow connections --add \
        --conn_id databricks_default \
        --conn_type databricks \
        --conn_host dbc-caf9527b-e073.cloud.databricks.com:443 \
        --conn_extra token=${DB_TOKEN}

    airflow connections --add \
        --conn_id google_cloud_prio_a \
        --conn_type google_cloud_platform

    airflow connections --add \
        --conn_id google_cloud_prio_b \
        --conn_type google_cloud_platform

    airflow connections --add \
        --conn_id google_cloud_derived_datasets \
        --conn_type google_cloud_platform

    airflow connections --add \
        --conn_id amplitude_s3_conn \
        --conn_type s3
}

[ $# -lt 1 ] && usage

# Only wait for backend services in development
# http://stackoverflow.com/a/13864829
[ ! -z ${DEVELOPMENT+check} ] && wait_for db 3306 && wait_for redis 6379

case $1 in
  flower)
    exec airflow flower
    ;;
  web)
    airflow initdb
    airflow upgradedb
    init_connections

    exec airflow webserver -p ${PORT} --workers 4
    ;;
  worker)
    exec airflow worker
    ;;
  scheduler)
    exec airflow scheduler
    ;;
  *)
    exec "$@"
    ;;
esac
